{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpCQBezcxJD1"
      },
      "source": [
        "# 02456 Molecular Property Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXmf6uiTxJD2"
      },
      "source": [
        "Basic example of how to train the PaiNN model to predict the QM9 property\n",
        "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
        "properties) is computed as a sum of atomic contributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6S8SmwK8xJD2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!pip install torch_geometric\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import argparse\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm import trange\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.transforms import BaseTransform\n",
        "from typing import Optional, List, Union, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcfhMz50xJD3"
      },
      "source": [
        "## QM9 Datamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vZY82V6DxJD4"
      },
      "outputs": [],
      "source": [
        "class GetTarget(BaseTransform):\n",
        "\n",
        "    def __init__(self, target: Optional[int] = None) -> None:\n",
        "        self.target = [target]\n",
        "\n",
        "\n",
        "    def forward(self, data: Data) -> Data:\n",
        "        if self.target is not None:\n",
        "            data.y = data.y[:, self.target]\n",
        "        return data\n",
        "\n",
        "\n",
        "class QM9DataModule(pl.LightningDataModule):\n",
        "\n",
        "    target_types = ['atomwise' for _ in range(19)]\n",
        "    target_types[0] = 'dipole_moment'\n",
        "    target_types[5] = 'electronic_spatial_extent'\n",
        "\n",
        "    # Specify unit conversions (eV to meV).\n",
        "    unit_conversion = {\n",
        "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
        "        else (lambda t: t)\n",
        "        for i in range(19)\n",
        "    }\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        target: int = 7,\n",
        "        data_dir: str = 'data/',\n",
        "        batch_size_train: int = 100,\n",
        "        batch_size_inference: int = 1000,\n",
        "        num_workers: int = 0,\n",
        "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
        "        seed: int = 0,\n",
        "        subset_size: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.target = target\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size_train = batch_size_train\n",
        "        self.batch_size_inference = batch_size_inference\n",
        "        self.num_workers = num_workers\n",
        "        self.splits = splits\n",
        "        self.seed = seed\n",
        "        self.subset_size = subset_size\n",
        "\n",
        "        self.data_train = None\n",
        "        self.data_val = None\n",
        "        self.data_test = None\n",
        "\n",
        "\n",
        "    def prepare_data(self) -> None:\n",
        "        # Download data\n",
        "        QM9(root=self.data_dir)\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
        "\n",
        "        # Shuffle dataset\n",
        "        rng = np.random.default_rng(seed=self.seed)\n",
        "        dataset = dataset[rng.permutation(len(dataset))]\n",
        "\n",
        "        # Subset dataset\n",
        "        if self.subset_size is not None:\n",
        "            dataset = dataset[:self.subset_size]\n",
        "\n",
        "        # Split dataset\n",
        "        if all([type(split) == int for split in self.splits]):\n",
        "            split_sizes = self.splits\n",
        "        elif all([type(split) == float for split in self.splits]):\n",
        "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
        "\n",
        "        split_idx = np.cumsum(split_sizes)\n",
        "        self.data_train = dataset[:split_idx[0]]\n",
        "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
        "        self.data_test = dataset[split_idx[1]:]\n",
        "\n",
        "\n",
        "    def get_target_stats(\n",
        "        self,\n",
        "        remove_atom_refs: bool = True,\n",
        "        divide_by_atoms: bool = True\n",
        "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
        "        atom_refs = self.data_train.atomref(self.target)                        # Atom reference energy\n",
        "\n",
        "        ys = list()\n",
        "        for batch in self.train_dataloader(shuffle=False):\n",
        "            y = batch.y.clone()\n",
        "            if remove_atom_refs and atom_refs is not None:\n",
        "                y.index_add_(\n",
        "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
        "                )\n",
        "            if divide_by_atoms:                                                 # Normalize internal energy by the number of atoms\n",
        "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
        "                y = y / num_atoms.unsqueeze(-1)\n",
        "            ys.append(y)\n",
        "\n",
        "        y = torch.cat(ys, dim=0)\n",
        "        return y.mean(), y.std(), atom_refs\n",
        "\n",
        "\n",
        "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.data_train,\n",
        "            batch_size=self.batch_size_train,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=shuffle,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.data_val,\n",
        "            batch_size=self.batch_size_inference,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.data_test,\n",
        "            batch_size=self.batch_size_inference,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Oevac1xJD5"
      },
      "source": [
        "## Post-processing module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "F3WkWe76xJD5"
      },
      "outputs": [],
      "source": [
        "class AtomwisePostProcessing(nn.Module):\n",
        "    \"\"\"\n",
        "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
        "    contributions.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_outputs: int,\n",
        "        mean: torch.FloatTensor,\n",
        "        std: torch.FloatTensor,\n",
        "        atom_refs: torch.FloatTensor,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_outputs: Integer with the number of model outputs. In most\n",
        "                cases 1.\n",
        "            mean: torch.FloatTensor with mean value to shift atomwise\n",
        "                contributions by.\n",
        "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
        "                contributions by.\n",
        "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
        "                atomic reference values.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "        self.register_buffer('scale', std)\n",
        "        self.register_buffer('shift', mean)\n",
        "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        atomic_contributions: torch.FloatTensor,\n",
        "        atoms: torch.LongTensor,\n",
        "        graph_indexes: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Atomwise post-processing operations and atomic sum.\n",
        "\n",
        "        Args:\n",
        "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
        "                num_outputs] with each node's contribution to the overall graph\n",
        "                prediction, i.e., each atom's contribution to the overall\n",
        "                molecular property prediction.\n",
        "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
        "                node in the graph.\n",
        "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph\n",
        "                index each node belongs to.\n",
        "\n",
        "        Returns:\n",
        "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
        "            predictions for each graph (molecule).\n",
        "        \"\"\"\n",
        "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
        "\n",
        "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
        "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
        "\n",
        "        # Sum contributions for each graph\n",
        "        output_per_graph = torch.zeros(\n",
        "            (num_graphs, self.num_outputs),\n",
        "            device=atomic_contributions.device,\n",
        "        )\n",
        "        output_per_graph.index_add_(\n",
        "            dim=0,\n",
        "            index=graph_indexes,\n",
        "            source=atomic_contributions,\n",
        "        )\n",
        "\n",
        "        return output_per_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1-c0ITwxJD6"
      },
      "source": [
        "## PaiNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def LocalEdges(atom_positions,\n",
        "               graph_indexes,\n",
        "               cutoff_dist):\n",
        "\n",
        "    # The number of atoms in the batch.\n",
        "    num_atoms = graph_indexes.size(0)\n",
        "\n",
        "    # Pairing of the atoms across all molecules in the batch.\n",
        "    pos_i = atom_positions.unsqueeze(0).repeat(num_atoms, 1, 1)\n",
        "    pos_j = atom_positions.unsqueeze(1).repeat(1, num_atoms, 1)\n",
        "\n",
        "    # Compute all r_ij vectors and their norms (distances).\n",
        "    r_ij = pos_j - pos_i                                          # Pairwise vector differences.\n",
        "    r_ij_norm = torch.norm(r_ij, dim=2)                           # Pairwise distances.\n",
        "\n",
        "    # We will not consider the distance between an atom and itself (i == j).\n",
        "    # We only consider atoms within the same molecule (graph_indexes[i] == graph_indexes[j]).\n",
        "    # We only want the pairs of close atoms specified by the cutoff.\n",
        "    # Thus, we create masks to filter pairs.\n",
        "    same_graph_mask     = graph_indexes.unsqueeze(0) == graph_indexes.unsqueeze(1)\n",
        "    different_atom_mask = torch.arange(num_atoms).unsqueeze(1) != torch.arange(num_atoms).unsqueeze(0)\n",
        "    within_cutoff_mask  = r_ij_norm <= cutoff_dist\n",
        "\n",
        "    # Combine masks: same graph, different atoms, within cutoff.\n",
        "    valid_pairs_mask = same_graph_mask & different_atom_mask & within_cutoff_mask\n",
        "\n",
        "    # Filter indices and values based on the mask.\n",
        "    edge_indexes = valid_pairs_mask.nonzero(as_tuple=False).t()   # Edge indexes, shape: (2, num_edges) - nonzero returns the indices of the elements that are non-zero (False is interpreted as 0).\n",
        "    edge_vector = r_ij[valid_pairs_mask]                          # Edge vectors, shape: (num_edges, 3)\n",
        "    edge_distance = r_ij_norm[valid_pairs_mask]                   # Edge distances, shape: (num_edges,)\n",
        "\n",
        "    return edge_indexes, edge_vector, edge_distance\n",
        "\n",
        "def RadialBasis(edge_distance,\n",
        "                num_rbf_features,\n",
        "                cutoff_dist):\n",
        "\n",
        "    # Number of local edges.\n",
        "    num_egdes = edge_distance.size()[0]\n",
        "\n",
        "    # Generate n values evenly spaced between 1 and 20.\n",
        "    n_values = torch.linspace(1, 20, num_rbf_features)\n",
        "\n",
        "    # Expand the n_values to match the shape of edge_distance.\n",
        "    n_values_expanded = n_values.unsqueeze(0).expand(num_egdes, num_rbf_features)\n",
        "    edge_distance_expanded = edge_distance.unsqueeze(1).expand(num_egdes, num_rbf_features)\n",
        "\n",
        "    # Compute the RBF for each pair of (r_ij, n).\n",
        "    edge_rbf = torch.sin(n_values_expanded * torch.pi * edge_distance_expanded / cutoff_dist) / edge_distance_expanded\n",
        "\n",
        "    return edge_rbf\n",
        "\n",
        "def CosineCutoff(edge_distance,\n",
        "                 cutoff_dist):\n",
        "\n",
        "    # Compute values of cutoff function.\n",
        "    fcut = 0.5 * (torch.cos(edge_distance * math.pi / cutoff_dist) + 1.0)\n",
        "\n",
        "    # Remove contributions beyond the cutoff radius.\n",
        "    fcut *= (fcut < cutoff_dist).float()\n",
        "\n",
        "    return fcut"
      ],
      "metadata": {
        "id": "RFYhPsszAFOd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "EAV3BSZoxJD7"
      },
      "outputs": [],
      "source": [
        "class MessageBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               num_features,\n",
        "               num_rbf_features):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_features = num_features\n",
        "\n",
        "    self.num_rbf_features = num_rbf_features\n",
        "\n",
        "    self.linear_s = nn.Sequential(\n",
        "        nn.Linear(num_features, num_features),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(num_features, num_features * 3),\n",
        "        )\n",
        "\n",
        "    self.linear_rbf = nn.Linear(num_rbf_features, num_features * 3)\n",
        "\n",
        "\n",
        "  def forward(self,\n",
        "              s,\n",
        "              vec,\n",
        "              edge_indexes,\n",
        "              edge_vector,\n",
        "              edge_distance,\n",
        "              cutoff_dist):\n",
        "\n",
        "    # Atomwise layers.\n",
        "    phi = self.linear_s(s)\n",
        "\n",
        "    # Compute radial basis functions.\n",
        "    edge_rbf = RadialBasis(edge_distance,\n",
        "                           self.num_features,\n",
        "                           cutoff_dist)\n",
        "\n",
        "    # Linear combination of the radial basis functions.\n",
        "    edge_rbf_linear = self.linear_rbf(edge_rbf)\n",
        "\n",
        "    # Cosine cutoff.\n",
        "    fcut = CosineCutoff(edge_distance,\n",
        "                        cutoff_dist)\n",
        "\n",
        "    W = edge_rbf_linear * fcut[..., None]\n",
        "\n",
        "    # Split of W.\n",
        "    vec_Ws, vec_Wvv, vec_Wvs = torch.split(phi * W, self.num_features, -1)\n",
        "\n",
        "    # Aggregate contributions from neighboring atoms\n",
        "    ds = vec_Ws\n",
        "\n",
        "    vec_n = edge_vector / edge_distance[..., None]\n",
        "\n",
        "    dvec = vec * Wvv.unsqueeze(1) + Wvs * vec_n.unsqueeze(2)\n",
        "\n",
        "    # Aggregate contributions from neighboring atoms\n",
        "\n",
        "    return ds, dvec\n",
        "\n",
        "class UpdateBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.linear_vec = nn.Linear(num_features, num_features * 2, bias=False)\n",
        "\n",
        "        self.linear_svec = nn.Sequential(\n",
        "            nn.Linear(num_features * 2, num_features),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(num_features, num_features * 3),\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                s,\n",
        "                vec):\n",
        "\n",
        "        vec_U, vec_V = torch.split(self.linear_vec(vec), self.num_features, dim = -1)\n",
        "\n",
        "        vec_dot = (vec_U * vec_V).sum(dim=1) #* self.inv_sqrt_h\n",
        "\n",
        "        vec_Vn = torch.sqrt(torch.sum(vec_V**2, dim = -2) + 1e-8)      # Add an epsilon offset to make sure sqrt is always positive.\n",
        "\n",
        "        vec_W = self.linear_svec(torch.cat([s, vec_Vn], dim = -1))\n",
        "\n",
        "        a_vv, a_sv, a_ss = torch.split(vec_W, self.num_features, dim = -1)\n",
        "\n",
        "        ds = a_ss + a_sv * vec_dot    # * self.inv_sqrt_2\n",
        "\n",
        "        dvec = a_vv.unsqueeze(1) * vec_U\n",
        "\n",
        "        return ds, dvec\n",
        "\n",
        "class PaiNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_message_passing_layers: int = 3,\n",
        "        num_features: int = 128,\n",
        "        num_outputs: int = 1,\n",
        "        num_rbf_features: int = 20,\n",
        "        num_unique_atoms: int = 100,\n",
        "        cutoff_dist: float = 5.0,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_message_passing_layers: Number of message passing layers in\n",
        "                the PaiNN model.\n",
        "            num_features: Size of the node embeddings (scalar features) and\n",
        "                vector features.\n",
        "            num_outputs: Number of model outputs. In most cases 1.\n",
        "            num_rbf_features: Number of radial basis functions to represent\n",
        "                distances.\n",
        "            num_unique_atoms: Number of unique atoms in the data that we want\n",
        "                to learn embeddings for.\n",
        "            cutoff_dist: Euclidean distance threshold for determining whether\n",
        "                two nodes (atoms) are neighbours.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_message_passing_layers = num_message_passing_layers\n",
        "        self.num_features = num_features\n",
        "        self.num_outputs = num_outputs\n",
        "        self.num_rbf_features = num_rbf_features\n",
        "        self.num_unique_atoms = num_unique_atoms\n",
        "        self.cutoff_dist = cutoff_dist\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding_s = nn.Embedding(num_unique_atoms, num_features)\n",
        "        self.embedding_v = nn.Embedding(3, num_features, sparse=True)\n",
        "\n",
        "        self.message_1 = MessageBlock() #INSERT ARGUEMENTS\n",
        "        self.message_2 = MessageBlock() #INSERT ARGUEMENTS\n",
        "        self.message_3 = MessageBlock() #INSERT ARGUEMENTS\n",
        "\n",
        "        self.update_1 = UpdateBlock()\n",
        "        self.update_2 = UpdateBlock()\n",
        "        self.update_3 = UpdateBlock()\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(num_features, num_features // 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(num_features // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        atoms: torch.LongTensor,\n",
        "        atom_positions: torch.FloatTensor,\n",
        "        graph_indexes: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
        "        in Figure 2 in (SchÃ¼tt et al., 2021) with normal linear layers which is\n",
        "        used for predicting properties as sums of atomic contributions. The\n",
        "        post-processing and final sum is perfomed with\n",
        "        src.models.AtomwisePostProcessing.\n",
        "\n",
        "        Args:\n",
        "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
        "                node in the graph.\n",
        "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
        "                euclidean coordinates of each node / atom.\n",
        "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph\n",
        "                index each node belongs to.\n",
        "\n",
        "        Returns:\n",
        "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
        "            contributions to the overall molecular property prediction.\n",
        "        \"\"\"\n",
        "        # ----------------------------------------------------------------------\n",
        "        # EMBEDDING\n",
        "        # We initialize learnable embeddings for the atomtype.\n",
        "        # The directions v_i are embedded by a null vector.\n",
        "\n",
        "        s = self.embedding_s(atoms)\n",
        "        v = self.embedding_v(atoms)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # LOCAL NEIGHBORHOOD\n",
        "        # We create edges by the relative position of nodes from a specified\n",
        "        # cutoff within the same molecule (local interactions)\n",
        "\n",
        "        edge_indexes, edge_vector, edge_distance = LocalEdges(atom_positions,\n",
        "                                                              graph_indexes,\n",
        "                                                              self.cutoff_dist)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # RADIAL BASIS\n",
        "\n",
        "        edge_rbf = RadialBasis(edge_distance,\n",
        "                               num_rbf_features,\n",
        "                               cutoff_dist)\n",
        "\n",
        "        # MESSAGE AND UPDATE\n",
        "\n",
        "        # Message and update 1\n",
        "        ds, dv = self.message_1()\n",
        "        s = s + ds\n",
        "        v = v + dv\n",
        "\n",
        "        ds, dv = self.update_1()\n",
        "        s = s + ds\n",
        "        v = v + dv\n",
        "\n",
        "        # Message and update 2\n",
        "        ds, dv = self.message_2()\n",
        "        s = s + ds\n",
        "        v = v + dv\n",
        "\n",
        "        ds, dv = self.update_2()\n",
        "        s = s + ds\n",
        "        v = v + dv\n",
        "\n",
        "        # Message and update 3\n",
        "        ds, dv = self.message_3()\n",
        "        s = s + ds\n",
        "        v = v + dv\n",
        "\n",
        "        ds, dv = self.update_3()\n",
        "        s = s + ds\n",
        "        v = v + dv\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # ATOMIC CONTRIBUTIONS\n",
        "\n",
        "        atomic_contributions = self.output(s)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        # Final output\n",
        "        return atomic_contributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n-d1iJtxJD8"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5VcXhSQ0xJD8"
      },
      "outputs": [],
      "source": [
        "def cli(args: list = []):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--seed', default=0)\n",
        "\n",
        "    # Data\n",
        "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
        "    parser.add_argument('--data_dir', default='data/', type=str)\n",
        "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
        "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
        "    parser.add_argument('--num_workers', default=0, type=int)\n",
        "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
        "    parser.add_argument('--subset_size', default=None, type=int)\n",
        "\n",
        "    # Model\n",
        "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
        "    parser.add_argument('--num_features', default=128, type=int)\n",
        "    parser.add_argument('--num_outputs', default=1, type=int)\n",
        "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
        "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
        "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
        "\n",
        "    # Training\n",
        "    parser.add_argument('--lr', default=5e-4, type=float)\n",
        "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
        "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
        "\n",
        "    args = parser.parse_args(args=args)\n",
        "    return args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xsfP4uYxJD8"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify non-default arguments in this list.\n",
        "args = []\n",
        "args = cli(args)\n",
        "seed_everything(args.seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load and prepare data from the QM9 data set.\n",
        "dm = QM9DataModule(\n",
        "    target=args.target,\n",
        "    data_dir=args.data_dir,\n",
        "    batch_size_train=args.batch_size_train,\n",
        "    batch_size_inference=args.batch_size_inference,\n",
        "    num_workers=args.num_workers,\n",
        "    splits=args.splits,\n",
        "    seed=args.seed,\n",
        "    subset_size=args.subset_size,\n",
        ")\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "\n",
        "# Calculate target statistics.\n",
        "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
        "    remove_atom_refs=True, divide_by_atoms=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTXpHMR1bmqt",
        "outputId": "d0b7a20e-9ee0-4d8c-a591-c6971a621eeb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 0\n",
            "Downloading https://data.pyg.org/datasets/qm9_v3.zip\n",
            "Extracting data/raw/qm9_v3.zip\n",
            "Processing...\n",
            "Using a pre-processed version of the dataset. Please install 'rdkit' to alternatively process the raw data.\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "IK2gdRmhxJD8",
        "outputId": "ec475309-c24d-4db0-f93a-87cf1d0d5507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 0\n",
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "MessagePassingLayer.forward() missing 1 required positional argument: 'edge_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b463b063ec92>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         atomic_contributions = painn(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0matoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0matom_positions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-aa9e89aaa4e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, atoms, atom_positions, graph_indexes)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Apply message passing layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0matom_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matom_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrbf_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Aggregate atomic features to get the molecule-level prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: MessagePassingLayer.forward() missing 1 required positional argument: 'edge_index'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize the model.\n",
        "painn = PaiNN(\n",
        "    num_message_passing_layers=args.num_message_passing_layers,     # 3\n",
        "    num_features=args.num_features,                                 # 128\n",
        "    num_outputs=args.num_outputs,                                   # 1\n",
        "    num_rbf_features=args.num_rbf_features,\n",
        "    num_unique_atoms=args.num_unique_atoms,\n",
        "    cutoff_dist=args.cutoff_dist,                                   # 5\n",
        ")\n",
        "\n",
        "post_processing = AtomwisePostProcessing(\n",
        "    args.num_outputs, y_mean, y_std, atom_refs\n",
        ")\n",
        "\n",
        "painn.to(device)\n",
        "post_processing.to(device)\n",
        "\n",
        "# Define optimizer.\n",
        "optimizer = torch.optim.AdamW(\n",
        "    painn.parameters(),\n",
        "    lr=args.lr,\n",
        "    weight_decay=args.weight_decay,\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "painn.train()\n",
        "pbar = trange(args.num_epochs)\n",
        "for epoch in pbar:\n",
        "\n",
        "    loss_epoch = 0.\n",
        "    for batch in dm.train_dataloader():\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        atomic_contributions = painn(\n",
        "            atoms=batch.z,\n",
        "            atom_positions=batch.pos,\n",
        "            graph_indexes=batch.batch\n",
        "        )\n",
        "        preds = post_processing(\n",
        "            atoms=batch.z,\n",
        "            graph_indexes=batch.batch,\n",
        "            atomic_contributions=atomic_contributions,\n",
        "        )\n",
        "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
        "\n",
        "        loss = loss_step / len(batch.y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss_step.detach().item()\n",
        "    loss_epoch /= len(dm.data_train)\n",
        "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
        "\n",
        "mae = 0\n",
        "painn.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in dm.test_dataloader():\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        atomic_contributions = painn(\n",
        "            atoms=batch.z,\n",
        "            atom_positions=batch.pos,\n",
        "            graph_indexes=batch.batch,\n",
        "        )\n",
        "        preds = post_processing(\n",
        "            atoms=batch.z,\n",
        "            graph_indexes=batch.batch,\n",
        "            atomic_contributions=atomic_contributions,\n",
        "        )\n",
        "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
        "\n",
        "mae /= len(dm.data_test)\n",
        "unit_conversion = dm.unit_conversion[args.target]\n",
        "print(f'Test MAE: {unit_conversion(mae):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoPQa1tPxJD9",
        "outputId": "8f380af6-1cf5-49cb-ff33-fdb8df3e4656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 110000\n",
            "Validation set size: 10000\n",
            "Test set size: 10831\n",
            "Sample features: Data(x=[13, 11], edge_index=[2, 26], edge_attr=[26, 4], y=[1, 1], pos=[13, 3], idx=[1], name='gdb_2329', z=[13])\n",
            "Atom type (z): tensor([8, 6, 6, 6, 8, 6, 8, 1, 1, 1, 1, 1, 1])\n",
            "Atom position (pos): tensor([[-0.4970,  1.2608, -0.4083],\n",
            "        [-0.2214, -0.0731, -0.1197],\n",
            "        [-0.3092, -0.6333,  1.2750],\n",
            "        [-1.3466, -1.0139,  0.2482],\n",
            "        [-2.6112, -0.4155,  0.4511],\n",
            "        [-3.2778, -0.0487, -0.6749],\n",
            "        [-4.3477,  0.4762, -0.6321],\n",
            "        [-1.0897,  1.6111,  0.2655],\n",
            "        [ 0.5573, -0.4431, -0.7776],\n",
            "        [ 0.4061, -1.3830,  1.5939],\n",
            "        [-0.6500,  0.0318,  2.0625],\n",
            "        [-1.3897, -2.0286, -0.1362],\n",
            "        [-2.7250, -0.2967, -1.5982]])\n",
            "Edge indices (edge_index): tensor([[ 0,  0,  1,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  3,  4,  4,  5,  5,\n",
            "          5,  6,  7,  8,  9, 10, 11, 12],\n",
            "        [ 1,  7,  0,  2,  3,  8,  1,  3,  9, 10,  1,  2,  4, 11,  3,  5,  4,  6,\n",
            "         12,  5,  0,  1,  2,  2,  3,  5]])\n",
            "Target properties (y): tensor([[-10383.3389]])\n",
            "Target mean: tensor(-4.2433)\n",
            "Target standard deviation: tensor(0.1888)\n"
          ]
        }
      ],
      "source": [
        "# Check sizes of train, validation, and test splits:\n",
        "print(\"Training set size:\", len(dm.data_train))\n",
        "print(\"Validation set size:\", len(dm.data_val))\n",
        "print(\"Test set size:\", len(dm.data_test))\n",
        "\n",
        "# View the first sample in the training dataset:\n",
        "sample = dm.data_train[0]\n",
        "print(\"Sample features:\", sample)\n",
        "\n",
        "# Access individual attributes of the sample which we will use:\n",
        "print(\"Atom type (z):\", sample.z)                       # Atom type for each node in the graph\n",
        "print(\"Atom position (pos):\", sample.pos)               # Atom position for each node in the graph\n",
        "print(\"Edge indices (edge_index):\", sample.edge_index)  # Connectivity info between atoms\n",
        "print(\"Target properties (y):\", sample.y)               # Target property (energy)\n",
        "\n",
        "# Print the mean and standard deviation for the target property\n",
        "print(\"Target mean:\", y_mean)\n",
        "print(\"Target standard deviation:\", y_std)\n",
        "\n",
        "# Print atom reference values (standardized contributions of individual atoms to internal energy)\n",
        "#print(\"Atom reference values:\", atom_refs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "painn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}