{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpCQBezcxJD1"
      },
      "source": [
        "# 02456 Molecular Property Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXmf6uiTxJD2"
      },
      "source": [
        "Basic example of how to train the PaiNN model to predict the QM9 property\n",
        "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
        "properties) is computed as a sum of atomic contributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S8SmwK8xJD2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install pytorch_lightning\n",
        "!pip install torch_geometric\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import argparse\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from tqdm import trange\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import QM9, QM7b\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.transforms import BaseTransform\n",
        "from typing import Optional, List, Union, Tuple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QM7b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IkGbkwTw1r2l",
        "outputId": "a8cbbd6c-82b0-412e-a9ae-08cd8284dbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch_geometric.datasets.qm7.QM7b"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torch_geometric.datasets.qm7.QM7b</b><br/>def __init__(root: str, transform: Optional[Callable]=None, pre_transform: Optional[Callable]=None, pre_filter: Optional[Callable]=None, force_reload: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/torch_geometric/datasets/qm7.py</a>The QM7b dataset from the `&quot;MoleculeNet: A Benchmark for Molecular\n",
              "Machine Learning&quot; &lt;https://arxiv.org/abs/1703.00564&gt;`_ paper, consisting of\n",
              "7,211 molecules with 14 regression targets.\n",
              "\n",
              "Args:\n",
              "    root (str): Root directory where the dataset should be saved.\n",
              "    transform (callable, optional): A function/transform that takes in an\n",
              "        :obj:`torch_geometric.data.Data` object and returns a transformed\n",
              "        version. The data object will be transformed before every access.\n",
              "        (default: :obj:`None`)\n",
              "    pre_transform (callable, optional): A function/transform that takes in\n",
              "        an :obj:`torch_geometric.data.Data` object and returns a\n",
              "        transformed version. The data object will be transformed before\n",
              "        being saved to disk. (default: :obj:`None`)\n",
              "    pre_filter (callable, optional): A function that takes in an\n",
              "        :obj:`torch_geometric.data.Data` object and returns a boolean\n",
              "        value, indicating whether the data object should be included in the\n",
              "        final dataset. (default: :obj:`None`)\n",
              "    force_reload (bool, optional): Whether to re-process the dataset.\n",
              "        (default: :obj:`False`)\n",
              "\n",
              "**STATS:**\n",
              "\n",
              ".. list-table::\n",
              "    :widths: 10 10 10 10 10\n",
              "    :header-rows: 1\n",
              "\n",
              "    * - #graphs\n",
              "      - #nodes\n",
              "      - #edges\n",
              "      - #features\n",
              "      - #tasks\n",
              "    * - 7,211\n",
              "      - ~15.4\n",
              "      - ~245.0\n",
              "      - 0\n",
              "      - 14</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 8);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcfhMz50xJD3"
      },
      "source": [
        "## QM9 Datamodule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZY82V6DxJD4"
      },
      "outputs": [],
      "source": [
        "class GetTarget(BaseTransform):\n",
        "\n",
        "    def __init__(self, target: Optional[int] = None) -> None:\n",
        "        self.target = [target]\n",
        "\n",
        "\n",
        "    def forward(self, data: Data) -> Data:\n",
        "        if self.target is not None:\n",
        "            data.y = data.y[:, self.target]\n",
        "        return data\n",
        "\n",
        "\n",
        "class QM9DataModule(pl.LightningDataModule):\n",
        "\n",
        "    target_types = ['atomwise' for _ in range(19)]\n",
        "    target_types[0] = 'dipole_moment'\n",
        "    target_types[5] = 'electronic_spatial_extent'\n",
        "\n",
        "    # Specify unit conversions (eV to meV).\n",
        "    unit_conversion = {\n",
        "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
        "        else (lambda t: t)\n",
        "        for i in range(19)\n",
        "    }\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        target: int = 7,\n",
        "        data_dir: str = 'data/',\n",
        "        batch_size_train: int = 100,\n",
        "        batch_size_inference: int = 1000,\n",
        "        num_workers: int = 0,\n",
        "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
        "        seed: int = 0,\n",
        "        subset_size: Optional[int] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.target = target\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size_train = batch_size_train\n",
        "        self.batch_size_inference = batch_size_inference\n",
        "        self.num_workers = num_workers\n",
        "        self.splits = splits\n",
        "        self.seed = seed\n",
        "        self.subset_size = subset_size\n",
        "\n",
        "        self.data_train = None\n",
        "        self.data_val = None\n",
        "        self.data_test = None\n",
        "\n",
        "\n",
        "    def prepare_data(self) -> None:\n",
        "        # Download data\n",
        "        QM9(root=self.data_dir)\n",
        "\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None) -> None:\n",
        "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
        "\n",
        "        # Shuffle dataset\n",
        "        rng = np.random.default_rng(seed=self.seed)\n",
        "        dataset = dataset[rng.permutation(len(dataset))]\n",
        "\n",
        "        # Subset dataset\n",
        "        if self.subset_size is not None:\n",
        "            dataset = dataset[:self.subset_size]\n",
        "\n",
        "        # Split dataset\n",
        "        if all([type(split) == int for split in self.splits]):\n",
        "            split_sizes = self.splits\n",
        "        elif all([type(split) == float for split in self.splits]):\n",
        "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
        "\n",
        "        split_idx = np.cumsum(split_sizes)\n",
        "        self.data_train = dataset[:split_idx[0]]\n",
        "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
        "        self.data_test = dataset[split_idx[1]:]\n",
        "\n",
        "\n",
        "    def get_target_stats(\n",
        "        self,\n",
        "        remove_atom_refs: bool = True,\n",
        "        divide_by_atoms: bool = True\n",
        "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
        "        atom_refs = self.data_train.atomref(self.target)                        # Atom reference energy\n",
        "\n",
        "        ys = list()\n",
        "        for batch in self.train_dataloader(shuffle=False):\n",
        "            y = batch.y.clone()\n",
        "            if remove_atom_refs and atom_refs is not None:\n",
        "                y.index_add_(\n",
        "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
        "                )\n",
        "            if divide_by_atoms:                                                 # Normalize internal energy by the number of atoms\n",
        "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
        "                y = y / num_atoms.unsqueeze(-1)\n",
        "            ys.append(y)\n",
        "\n",
        "        y = torch.cat(ys, dim=0)\n",
        "        return y.mean(), y.std(), atom_refs\n",
        "\n",
        "\n",
        "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.data_train,\n",
        "            batch_size=self.batch_size_train,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=shuffle,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.data_val,\n",
        "            batch_size=self.batch_size_inference,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.data_test,\n",
        "            batch_size=self.batch_size_inference,\n",
        "            num_workers=self.num_workers,\n",
        "            shuffle=False,\n",
        "            pin_memory=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Oevac1xJD5"
      },
      "source": [
        "## Post-processing module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3WkWe76xJD5"
      },
      "outputs": [],
      "source": [
        "class AtomwisePostProcessing(nn.Module):\n",
        "    \"\"\"\n",
        "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
        "    contributions.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_outputs: int,\n",
        "        mean: torch.FloatTensor,\n",
        "        std: torch.FloatTensor,\n",
        "        atom_refs: torch.FloatTensor,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_outputs: Integer with the number of model outputs. In most\n",
        "                cases 1.\n",
        "            mean: torch.FloatTensor with mean value to shift atomwise\n",
        "                contributions by.\n",
        "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
        "                contributions by.\n",
        "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
        "                atomic reference values.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "        self.register_buffer('scale', std)\n",
        "        self.register_buffer('shift', mean)\n",
        "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        atomic_contributions: torch.FloatTensor,\n",
        "        atoms: torch.LongTensor,\n",
        "        graph_indexes: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Atomwise post-processing operations and atomic sum.\n",
        "\n",
        "        Args:\n",
        "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
        "                num_outputs] with each node's contribution to the overall graph\n",
        "                prediction, i.e., each atom's contribution to the overall\n",
        "                molecular property prediction.\n",
        "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
        "                node in the graph.\n",
        "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph\n",
        "                index each node belongs to.\n",
        "\n",
        "        Returns:\n",
        "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
        "            predictions for each graph (molecule).\n",
        "        \"\"\"\n",
        "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
        "\n",
        "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
        "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
        "\n",
        "        # Sum contributions for each graph\n",
        "        output_per_graph = torch.zeros(\n",
        "            (num_graphs, self.num_outputs),\n",
        "            device=atomic_contributions.device,\n",
        "        )\n",
        "        output_per_graph.index_add_(\n",
        "            dim=0,\n",
        "            index=graph_indexes,\n",
        "            source=atomic_contributions,\n",
        "        )\n",
        "\n",
        "        return output_per_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1-c0ITwxJD6"
      },
      "source": [
        "## PaiNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFYhPsszAFOd"
      },
      "outputs": [],
      "source": [
        "def LocalEdges(atom_positions,\n",
        "               graph_indexes,\n",
        "               cutoff_dist):\n",
        "\n",
        "    # The number of atoms in the batch.\n",
        "    num_atoms = graph_indexes.size(0)\n",
        "\n",
        "    # Pairing of the atoms across all molecules in the batch.\n",
        "    pos_i = atom_positions.unsqueeze(0).repeat(num_atoms, 1, 1)\n",
        "    pos_j = atom_positions.unsqueeze(1).repeat(1, num_atoms, 1)\n",
        "\n",
        "    # Compute all r_ij vectors and their norms (distances).\n",
        "    r_ij = pos_j - pos_i                                          # Pairwise vector differences.\n",
        "    r_ij_norm = torch.norm(r_ij, dim=2)                           # Pairwise distances.\n",
        "\n",
        "    # We will not consider the distance between an atom and itself (i == j).\n",
        "    # We only consider atoms within the same molecule (graph_indexes[i] == graph_indexes[j]).\n",
        "    # We only want the pairs of close atoms specified by the cutoff.\n",
        "    # Thus, we create masks to filter pairs.\n",
        "    same_graph_mask     = graph_indexes.unsqueeze(0) == graph_indexes.unsqueeze(1)\n",
        "    different_atom_mask = torch.arange(num_atoms).unsqueeze(1) != torch.arange(num_atoms).unsqueeze(0)\n",
        "    within_cutoff_mask  = r_ij_norm <= cutoff_dist\n",
        "\n",
        "    # Combine masks: same graph, different atoms, within cutoff.\n",
        "    valid_pairs_mask = same_graph_mask & different_atom_mask & within_cutoff_mask\n",
        "\n",
        "    # Filter indices and values based on the mask.\n",
        "    edge_indexes = valid_pairs_mask.nonzero(as_tuple=False).t()   # Edge indexes, shape: (2, num_edges) - nonzero returns the indices of the elements that are non-zero (False is interpreted as 0).\n",
        "    edge_vector = r_ij[valid_pairs_mask]                          # Edge vectors, shape: (num_edges, 3)\n",
        "    edge_distance = r_ij_norm[valid_pairs_mask]                   # Edge distances, shape: (num_edges, 1)\n",
        "\n",
        "    return edge_indexes, edge_vector, edge_distance\n",
        "\n",
        "def RadialBasis(edge_distance,\n",
        "                num_rbf_features,\n",
        "                cutoff_dist):\n",
        "\n",
        "    # Number of local edges.\n",
        "    num_egdes = edge_distance.size()[0]\n",
        "\n",
        "    # Generate n values evenly spaced between 1 and 20.\n",
        "    n_values = torch.linspace(1, 20, num_rbf_features)\n",
        "\n",
        "    # Expand the n_values to match the shape of edge_distance.\n",
        "    n_values_expanded = n_values.unsqueeze(0).expand(num_egdes, num_rbf_features)\n",
        "    edge_distance_expanded = edge_distance.unsqueeze(1).expand(num_egdes, num_rbf_features)\n",
        "\n",
        "    # Compute the RBF for each pair of (r_ij, n).\n",
        "    edge_rbf = torch.sin(n_values_expanded * torch.pi * edge_distance_expanded / cutoff_dist) / edge_distance_expanded\n",
        "\n",
        "    return edge_rbf\n",
        "\n",
        "def CosineCutoff(edge_distance,\n",
        "                 cutoff_dist):\n",
        "\n",
        "    # Compute values of cutoff function.\n",
        "    fcut = 0.5 * (torch.cos(edge_distance * math.pi / cutoff_dist) + 1.0)\n",
        "\n",
        "    # Remove contributions beyond the cutoff radius.\n",
        "    #fcut *= (fcut < cutoff_dist).float()\n",
        "\n",
        "    return fcut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAV3BSZoxJD7"
      },
      "outputs": [],
      "source": [
        "class MessageBlock(nn.Module):\n",
        "  def __init__(self,\n",
        "               num_features,\n",
        "               num_rbf_features):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_features = num_features\n",
        "\n",
        "    self.num_rbf_features = num_rbf_features\n",
        "\n",
        "    self.linear_s = nn.Sequential(\n",
        "        nn.Linear(num_features, num_features),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(num_features, num_features * 3),\n",
        "        )\n",
        "\n",
        "    self.linear_rbf = nn.Linear(num_rbf_features, num_features * 3)\n",
        "\n",
        "  def forward(self,\n",
        "              s,\n",
        "              vec,\n",
        "              edge_indexes,\n",
        "              edge_vector,\n",
        "              edge_distance,\n",
        "              edge_rbf,\n",
        "              cutoff_dist):\n",
        "\n",
        "    # Compute number of atoms (nodes) in batch.\n",
        "    num_atoms = s.size(0)\n",
        "\n",
        "    # Initialize ds and dvec.\n",
        "    ds = torch.zeros(num_atoms, self.num_features)\n",
        "    dvec = torch.zeros(num_atoms, 3, self.num_features)\n",
        "\n",
        "    # Let S be the neighbors of the neigboring pairs in the egde index vector.\n",
        "    # That is, S has the shape: num_edges x num_features (embedding)\n",
        "    # We do the same for vec, which has the shape: num_edges x 3 x num_features (embedding)\n",
        "    S = s[edge_indexes[1]]\n",
        "    Vec = vec[edge_indexes[1]]\n",
        "\n",
        "    # Atomwise layers.\n",
        "    phi = self.linear_s(S)\n",
        "\n",
        "    # Compute radial basis functions.\n",
        "    #edge_rbf = RadialBasis(edge_distance,\n",
        "                           #self.num_features,\n",
        "                           #cutoff_dist)\n",
        "\n",
        "    # Linear combination of the radial basis functions.\n",
        "    edge_rbf_linear = self.linear_rbf(edge_rbf)\n",
        "\n",
        "    # Cosine cutoff.\n",
        "    fcut = CosineCutoff(edge_distance,\n",
        "                        cutoff_dist)\n",
        "\n",
        "    W = edge_rbf_linear * fcut[..., None]\n",
        "\n",
        "    # Split of W.\n",
        "    vec_Ws, vec_Wvv, vec_Wvs = torch.split(phi * W, self.num_features, -1)\n",
        "\n",
        "    # Aggregate contributions from neighboring atoms ?????\n",
        "    ds = ds.index_add_(dim = 0,\n",
        "                       index = edge_indexes[0],\n",
        "                       source = vec_Ws,\n",
        "                       alpha=1)\n",
        "\n",
        "    vec_n = edge_vector / edge_distance[..., None]\n",
        "\n",
        "    #dVec = vec_Wvv.unsqueeze(1) * Vec.unsqueeze(2) + vec_n * vec_Wvs.unsqueeze(1)\n",
        "    #dVec = vec_Wvv * Vec + vec_n * vec_Wvs\n",
        "    dVec = vec_Wvv.unsqueeze(1) * Vec + vec_n.unsqueeze(2) * vec_Wvs.unsqueeze(1)\n",
        "\n",
        "    dvec = dvec.index_add_(dim = 0,\n",
        "                           index = edge_indexes[0],\n",
        "                           source = dVec,\n",
        "                           alpha=1)\n",
        "\n",
        "    return ds, dvec\n",
        "\n",
        "\n",
        "class UpdateBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_features = num_features\n",
        "\n",
        "        self.linear_vec = nn.Linear(num_features, num_features * 2, bias=False)\n",
        "\n",
        "        self.linear_svec = nn.Sequential(\n",
        "            nn.Linear(num_features * 2, num_features),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(num_features, num_features * 3),\n",
        "        )\n",
        "\n",
        "    def forward(self,\n",
        "                s,\n",
        "                vec):\n",
        "\n",
        "        vec_U, vec_V = torch.split(self.linear_vec(vec), self.num_features, dim = -1)\n",
        "\n",
        "        vec_dot = (vec_U * vec_V).sum(dim=1) #* self.inv_sqrt_h\n",
        "\n",
        "        vec_Vn = torch.sqrt(torch.sum(vec_V**2, dim = -2) + 1e-8)      # Add an epsilon offset to make sure sqrt is always positive.\n",
        "\n",
        "        vec_W = self.linear_svec(torch.cat([s, vec_Vn], dim = -1))\n",
        "\n",
        "        a_vv, a_sv, a_ss = torch.split(vec_W, self.num_features, dim = -1)\n",
        "\n",
        "        ds = a_ss + a_sv * vec_dot    # * self.inv_sqrt_2\n",
        "\n",
        "        dvec = a_vv.unsqueeze(1) * vec_U\n",
        "\n",
        "        return ds, dvec\n",
        "\n",
        "class PaiNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_message_passing_layers: int = 3,\n",
        "        num_features: int = 128,\n",
        "        num_outputs: int = 1,\n",
        "        num_rbf_features: int = 20,\n",
        "        num_unique_atoms: int = 100,\n",
        "        cutoff_dist: float = 5.0,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_message_passing_layers: Number of message passing layers in\n",
        "                the PaiNN model.\n",
        "            num_features: Size of the node embeddings (scalar features) and\n",
        "                vector features.\n",
        "            num_outputs: Number of model outputs. In most cases 1.\n",
        "            num_rbf_features: Number of radial basis functions to represent\n",
        "                distances.\n",
        "            num_unique_atoms: Number of unique atoms in the data that we want\n",
        "                to learn embeddings for.\n",
        "            cutoff_dist: Euclidean distance threshold for determining whether\n",
        "                two nodes (atoms) are neighbours.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_message_passing_layers = num_message_passing_layers\n",
        "        self.num_features = num_features\n",
        "        self.num_outputs = num_outputs\n",
        "        self.num_rbf_features = num_rbf_features\n",
        "        self.num_unique_atoms = num_unique_atoms\n",
        "        self.cutoff_dist = cutoff_dist\n",
        "        self.device = device\n",
        "\n",
        "        self.embedding_s = nn.Embedding(num_unique_atoms, num_features)\n",
        "\n",
        "        self.message = nn.ModuleList()\n",
        "        self.update = nn.ModuleList()\n",
        "\n",
        "        for i in range(num_message_passing_layers):\n",
        "            self.message.append(MessageBlock(num_features, num_rbf_features))\n",
        "            self.update.append(UpdateBlock(num_features))\n",
        "\n",
        "        #self.message_1 = MessageBlock(num_features, num_rbf_features)\n",
        "        #self.message_2 = MessageBlock(num_features, num_rbf_features)\n",
        "        #self.message_3 = MessageBlock(num_features, num_rbf_features)\n",
        "\n",
        "        #self.update_1 = UpdateBlock(num_features)\n",
        "        #self.update_2 = UpdateBlock(num_features)\n",
        "        #self.update_3 = UpdateBlock(num_features)\n",
        "\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(num_features, num_features // 2),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(num_features // 2, 1),\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        atoms: torch.LongTensor,\n",
        "        atom_positions: torch.FloatTensor,\n",
        "        graph_indexes: torch.LongTensor,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
        "        in Figure 2 in (Schütt et al., 2021) with normal linear layers which is\n",
        "        used for predicting properties as sums of atomic contributions. The\n",
        "        post-processing and final sum is perfomed with\n",
        "        src.models.AtomwisePostProcessing.\n",
        "\n",
        "        Args:\n",
        "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
        "                node in the graph.\n",
        "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
        "                euclidean coordinates of each node / atom.\n",
        "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph\n",
        "                index each node belongs to.\n",
        "\n",
        "        Returns:\n",
        "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
        "            contributions to the overall molecular property prediction.\n",
        "        \"\"\"\n",
        "        # ----------------------------------------------------------------------\n",
        "        # EMBEDDING\n",
        "        # We initialize learnable embeddings for the atomtype.\n",
        "        # The directions v_i are embedded by a null vector.\n",
        "\n",
        "        s = self.embedding_s(atoms)\n",
        "        #vec = self.embedding_v(atoms)\n",
        "        vec = torch.zeros(s.size(0), 3, s.size(1))\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # LOCAL NEIGHBORHOOD\n",
        "        # We create edges by the relative position of nodes from a specified\n",
        "        # cutoff within the same molecule (local interactions)\n",
        "\n",
        "        edge_indexes, edge_vector, edge_distance = LocalEdges(atom_positions,\n",
        "                                                              graph_indexes,\n",
        "                                                              self.cutoff_dist)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # RADIAL BASIS\n",
        "\n",
        "        edge_rbf = RadialBasis(edge_distance,\n",
        "                               self.num_rbf_features,\n",
        "                               self.cutoff_dist)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # MESSAGE AND UPDATE\n",
        "\n",
        "\n",
        "        for i in range(self.num_message_passing_layers):\n",
        "            ds, dvec = self.message[i](s, vec, edge_indexes, edge_vector, edge_distance, edge_rbf, self.cutoff_dist)\n",
        "            s = s + ds\n",
        "            vec = vec + dvec\n",
        "\n",
        "            ds, dvec = self.update[i](s, vec)\n",
        "            s = s + ds\n",
        "            vec = vec + dvec\n",
        "\n",
        "        # Message and update 1\n",
        "        #ds, dvec = self.message_1(s, vec, edge_indexes, edge_vector, edge_distance, edge_rbf, self.cutoff_dist)\n",
        "        #s = s + ds\n",
        "        #vec = vec + dvec\n",
        "\n",
        "        #ds, dvec = self.update_1(s, vec)\n",
        "        #s = s + ds\n",
        "        #vec = vec + dvec\n",
        "\n",
        "        # Message and update 2\n",
        "        #ds, dvec = self.message_2(s, vec, edge_indexes, edge_vector, edge_distance, edge_rbf, self.cutoff_dist)\n",
        "        #s = s + ds\n",
        "        #vec = vec + dvec\n",
        "\n",
        "        #ds, dvec = self.update_2(s, vec)\n",
        "        #s = s + ds\n",
        "        #vec = vec + dvec\n",
        "\n",
        "        # Message and update 3\n",
        "        #ds, dvec = self.message_3(s, vec, edge_indexes, edge_vector, edge_distance, edge_rbf, self.cutoff_dist)\n",
        "        #s = s + ds\n",
        "        #vec = vec + dvec\n",
        "\n",
        "        #ds, dvec = self.update_3(s, vec)\n",
        "        #s = s + ds\n",
        "        #vec = vec + dvec\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "        # ATOMIC CONTRIBUTIONS\n",
        "\n",
        "        atomic_contributions = self.output(s)\n",
        "\n",
        "        # ----------------------------------------------------------------------\n",
        "\n",
        "        # Final output\n",
        "        return atomic_contributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n-d1iJtxJD8"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VcXhSQ0xJD8"
      },
      "outputs": [],
      "source": [
        "def cli(args: list = []):\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--seed', default=0)\n",
        "\n",
        "    # Data\n",
        "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
        "    parser.add_argument('--data_dir', default='data/', type=str)\n",
        "    parser.add_argument('--batch_size_train', default=10, type=int) #100\n",
        "    parser.add_argument('--batch_size_inference', default=10, type=int) #1000\n",
        "    parser.add_argument('--num_workers', default=2, type=int)\n",
        "    parser.add_argument('--splits', nargs=3, default=[900, 50, 50], type=int) # [num_train, num_val, num_test] #default=[110000, 10000, 10831]\n",
        "    parser.add_argument('--subset_size', default=1000, type=int) #None\n",
        "\n",
        "    # Model\n",
        "    parser.add_argument('--num_message_passing_layers', default=10, type=int)\n",
        "    parser.add_argument('--num_features', default=128, type=int)\n",
        "    parser.add_argument('--num_outputs', default=1, type=int)\n",
        "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
        "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
        "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
        "\n",
        "    # Training\n",
        "    parser.add_argument('--lr', default=5e-4, type=float) #5e-4\n",
        "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
        "    parser.add_argument('--num_epochs', default=10, type=int) #1000\n",
        "\n",
        "    args = parser.parse_args(args=args)\n",
        "    return args"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xsfP4uYxJD8"
      },
      "source": [
        "## Training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTXpHMR1bmqt",
        "outputId": "0f114c01-cafe-49fd-fc54-b2a629d7af21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 0\n"
          ]
        }
      ],
      "source": [
        "# Specify non-default arguments in this list.\n",
        "args = []\n",
        "args = cli(args)\n",
        "seed_everything(args.seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load and prepare data from the QM9 data set.\n",
        "dm = QM9DataModule(\n",
        "    target=args.target,\n",
        "    data_dir=args.data_dir,\n",
        "    batch_size_train=args.batch_size_train,\n",
        "    batch_size_inference=args.batch_size_inference,\n",
        "    num_workers=args.num_workers,\n",
        "    splits=args.splits,\n",
        "    seed=args.seed,\n",
        "    subset_size=args.subset_size,\n",
        ")\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "\n",
        "# Calculate target statistics.\n",
        "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
        "    remove_atom_refs=True, divide_by_atoms=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK2gdRmhxJD8",
        "outputId": "db1cd863-b772-4f1d-cabb-ebd753bc60fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:47<00:00, 10.71s/it, Train loss: 2.798e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 541.302\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model.\n",
        "painn = PaiNN(\n",
        "    num_message_passing_layers=args.num_message_passing_layers,     # 3\n",
        "    num_features=args.num_features,                                 # 128\n",
        "    num_outputs=args.num_outputs,                                   # 1\n",
        "    num_rbf_features=args.num_rbf_features,\n",
        "    num_unique_atoms=args.num_unique_atoms,\n",
        "    cutoff_dist=args.cutoff_dist,                                   # 5\n",
        ")\n",
        "\n",
        "post_processing = AtomwisePostProcessing(\n",
        "    args.num_outputs, y_mean, y_std, atom_refs\n",
        ")\n",
        "\n",
        "painn.to(device)\n",
        "post_processing.to(device)\n",
        "\n",
        "# Define optimizer.\n",
        "optimizer = torch.optim.AdamW(\n",
        "    painn.parameters(),\n",
        "    lr=args.lr,\n",
        "    weight_decay=args.weight_decay,\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "painn.train()\n",
        "pbar = trange(args.num_epochs)\n",
        "for epoch in pbar:\n",
        "\n",
        "    loss_epoch = 0.\n",
        "    for batch in dm.train_dataloader():\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        atomic_contributions = painn(\n",
        "            atoms=batch.z,\n",
        "            atom_positions=batch.pos,\n",
        "            graph_indexes=batch.batch\n",
        "        )\n",
        "        preds = post_processing(\n",
        "            atoms=batch.z,\n",
        "            graph_indexes=batch.batch,\n",
        "            atomic_contributions=atomic_contributions,\n",
        "        )\n",
        "        loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
        "\n",
        "        loss = loss_step / len(batch.y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss_step.detach().item()\n",
        "    loss_epoch /= len(dm.data_train)\n",
        "    pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
        "\n",
        "mae = 0\n",
        "painn.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in dm.test_dataloader():\n",
        "        batch = batch.to(device)\n",
        "        atomic_contributions = painn(\n",
        "            atoms=batch.z,\n",
        "            atom_positions=batch.pos,\n",
        "            graph_indexes=batch.batch,\n",
        "        )\n",
        "        preds = post_processing(\n",
        "            atoms=batch.z,\n",
        "            graph_indexes=batch.batch,\n",
        "            atomic_contributions=atomic_contributions,\n",
        "        )\n",
        "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
        "\n",
        "mae /= len(dm.data_test)\n",
        "unit_conversion = dm.unit_conversion[args.target]\n",
        "print(f'Test MAE: {unit_conversion(mae):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dm.data_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1ckEPE7-TzP",
        "outputId": "59daf775-86c1-495d-c48a-032a9f807d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130711"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = list()\n",
        "\n",
        "for i in range(10):\n",
        "    # Initialize the model.\n",
        "  painn = PaiNN(\n",
        "      num_message_passing_layers=i+1,\n",
        "      num_features=args.num_features,                                 # 128\n",
        "      num_outputs=args.num_outputs,                                   # 1\n",
        "      num_rbf_features=args.num_rbf_features,\n",
        "      num_unique_atoms=args.num_unique_atoms,\n",
        "      cutoff_dist=args.cutoff_dist,                                   # 5\n",
        "  )\n",
        "\n",
        "  post_processing = AtomwisePostProcessing(\n",
        "      args.num_outputs, y_mean, y_std, atom_refs\n",
        "  )\n",
        "\n",
        "  painn.to(device)\n",
        "  post_processing.to(device)\n",
        "\n",
        "  # Define optimizer.\n",
        "  optimizer = torch.optim.AdamW(\n",
        "      painn.parameters(),\n",
        "      lr=args.lr,\n",
        "      weight_decay=args.weight_decay,\n",
        "  )\n",
        "\n",
        "  # Train the model.\n",
        "  painn.train()\n",
        "  pbar = trange(args.num_epochs)\n",
        "  for epoch in pbar:\n",
        "\n",
        "      loss_epoch = 0.\n",
        "      for batch in dm.train_dataloader():\n",
        "          batch = batch.to(device)\n",
        "\n",
        "          atomic_contributions = painn(\n",
        "              atoms=batch.z,\n",
        "              atom_positions=batch.pos,\n",
        "              graph_indexes=batch.batch\n",
        "          )\n",
        "          preds = post_processing(\n",
        "              atoms=batch.z,\n",
        "              graph_indexes=batch.batch,\n",
        "              atomic_contributions=atomic_contributions,\n",
        "          )\n",
        "          loss_step = F.mse_loss(preds, batch.y, reduction='sum')\n",
        "\n",
        "          loss = loss_step / len(batch.y)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss_epoch += loss_step.detach().item()\n",
        "      loss_epoch /= len(dm.data_train)\n",
        "      pbar.set_postfix_str(f'Train loss: {loss_epoch:.3e}')\n",
        "\n",
        "  mae = 0\n",
        "  painn.eval()\n",
        "  with torch.no_grad():\n",
        "      for batch in dm.test_dataloader():\n",
        "          batch = batch.to(device)\n",
        "          atomic_contributions = painn(\n",
        "              atoms=batch.z,\n",
        "              atom_positions=batch.pos,\n",
        "              graph_indexes=batch.batch,\n",
        "          )\n",
        "          preds = post_processing(\n",
        "              atoms=batch.z,\n",
        "              graph_indexes=batch.batch,\n",
        "              atomic_contributions=atomic_contributions,\n",
        "          )\n",
        "          mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
        "\n",
        "  mae /= len(dm.data_test)\n",
        "  unit_conversion = dm.unit_conversion[args.target]\n",
        "  print(f'Test MAE: {unit_conversion(mae):.3f}')\n",
        "\n",
        "  res.append(mae)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgMGQ1lt9YpG",
        "outputId": "8aacb18d-09a7-46bd-fbf1-7d86e2b3052f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:26<00:00,  2.61s/it, Train loss: 2.830e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 448.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:35<00:00,  3.55s/it, Train loss: 2.353e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 412.044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:44<00:00,  4.43s/it, Train loss: 4.881e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 535.525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:53<00:00,  5.34s/it, Train loss: 3.926e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 686.602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:02<00:00,  6.25s/it, Train loss: 2.439e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 519.420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:12<00:00,  7.26s/it, Train loss: 3.090e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 688.450\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:19<00:00,  7.99s/it, Train loss: 6.266e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 735.143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:29<00:00,  8.93s/it, Train loss: 4.914e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 474.736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:38<00:00,  9.89s/it, Train loss: 6.442e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 603.465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:45<00:00, 10.59s/it, Train loss: 3.365e-01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MAE: 512.120\n",
            "[tensor(0.4482), tensor(0.4120), tensor(0.5355), tensor(0.6866), tensor(0.5194), tensor(0.6884), tensor(0.7351), tensor(0.4747), tensor(0.6035), tensor(0.5121)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preds)\n",
        "print(batch.y)"
      ],
      "metadata": {
        "id": "OzOm5lYT93gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoPQa1tPxJD9"
      },
      "outputs": [],
      "source": [
        "# Check sizes of train, validation, and test splits:\n",
        "#print(\"Training set size:\", len(dm.data_train))\n",
        "#print(\"Validation set size:\", len(dm.data_val))\n",
        "#print(\"Test set size:\", len(dm.data_test))\n",
        "\n",
        "# View the first sample in the training dataset:\n",
        "#sample = dm.data_train[0]\n",
        "#print(\"Sample features:\", sample)\n",
        "\n",
        "# Access individual attributes of the sample which we will use:\n",
        "#print(\"Atom type (z):\", sample.z)                       # Atom type for each node in the graph\n",
        "#print(\"Atom position (pos):\", sample.pos)               # Atom position for each node in the graph\n",
        "#print(\"Edge indices (edge_index):\", sample.edge_index)  # Connectivity info between atoms\n",
        "#print(\"Target properties (y):\", sample.y)               # Target property (energy)\n",
        "\n",
        "# Print the mean and standard deviation for the target property\n",
        "#print(\"Target mean:\", y_mean)\n",
        "#print(\"Target standard deviation:\", y_std)\n",
        "\n",
        "# Print atom reference values (standardized contributions of individual atoms to internal energy)\n",
        "#print(\"Atom reference values:\", atom_refs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "painn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}